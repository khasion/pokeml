{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Who is that Pokemon?!</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Parameters\n",
    "TARGET_SIZE = (128, 128)\n",
    "HOG_ORIENTATIONS = 9\n",
    "HOG_PIXELS_PER_CELL = (8, 8)\n",
    "HOG_CELLS_PER_BLOCK = (2, 2)\n",
    "COLOR_BINS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Set Up and Load the Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for binary classification (Pokémon vs. No-Pokémon)\n",
    "def load_binary_data(pokemon_folder, no_pokemon_folder):\n",
    "    images, labels = [], []\n",
    "    # Load Pokémon images (label 1)\n",
    "    for class_name in os.listdir(pokemon_folder):\n",
    "        class_path = os.path.join(pokemon_folder, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_file in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, TARGET_SIZE)\n",
    "                    images.append(img)\n",
    "                    labels.append(1)\n",
    "    # Load No-Pokémon images (label 0)\n",
    "    for img_file in os.listdir(no_pokemon_folder):\n",
    "        img_path = os.path.join(no_pokemon_folder, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, TARGET_SIZE)\n",
    "            images.append(img)\n",
    "            labels.append(0)\n",
    "    return images, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for multi-class classification\n",
    "def load_multiclass_data(pokemon_folder):\n",
    "    images, labels = [], []\n",
    "    for class_name in os.listdir(pokemon_folder):\n",
    "        class_path = os.path.join(pokemon_folder, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_file in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, TARGET_SIZE)\n",
    "                    images.append(img)\n",
    "                    labels.append(class_name)\n",
    "    return images, np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "def extract_features(images):\n",
    "    features_list = []\n",
    "    for img in images:\n",
    "        # Original HOG Features\n",
    "        hog_feat = hog(img, orientations=HOG_ORIENTATIONS,\n",
    "                      pixels_per_cell=HOG_PIXELS_PER_CELL,\n",
    "                      cells_per_block=HOG_CELLS_PER_BLOCK,\n",
    "                      channel_axis=-1)\n",
    "        \n",
    "        # LBP Features (Texture)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        lbp = local_binary_pattern(gray, P=8, R=1, method='uniform')\n",
    "        lbp_hist, _ = np.histogram(lbp.ravel(), bins=256, range=(0, 256))\n",
    "        \n",
    "        # Improved Color Features using HSV\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        hist_h = np.histogram(hsv[:,:,0], bins=COLOR_BINS, range=(0,180))[0]\n",
    "        hist_s = np.histogram(hsv[:,:,1], bins=COLOR_BINS, range=(0,256))[0]\n",
    "        hist_v = np.histogram(hsv[:,:,2], bins=COLOR_BINS, range=(0,256))[0]\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_feat = np.concatenate([\n",
    "            hog_feat,\n",
    "            lbp_hist,\n",
    "            hist_h,\n",
    "            hist_s,\n",
    "            hist_v\n",
    "        ])\n",
    "        features_list.append(combined_feat)\n",
    "    \n",
    "    return np.array(features_list)\n",
    "\n",
    "def create_sift_features(images, n_clusters=50):\n",
    "    sift = cv2.SIFT_create()\n",
    "    descriptors = []\n",
    "    \n",
    "    for img in images:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        _, des = sift.detectAndCompute(gray, None)\n",
    "        if des is not None:\n",
    "            descriptors.append(des)\n",
    "    \n",
    "    # Flatten descriptors and cluster\n",
    "    all_descriptors = np.vstack(descriptors)\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(all_descriptors)\n",
    "    \n",
    "    # Create histograms\n",
    "    sift_features = []\n",
    "    for img in images:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        _, des = sift.detectAndCompute(gray, None)\n",
    "        if des is None:\n",
    "            hist = np.zeros(n_clusters)\n",
    "        else:\n",
    "            clusters = kmeans.predict(des)\n",
    "            hist = np.bincount(clusters, minlength=n_clusters)\n",
    "        sift_features.append(hist)\n",
    "    \n",
    "    return np.array(sift_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\i.kasionis\\AppData\\Local\\anaconda3\\envs\\msc\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1955: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 2560 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classifier Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       580\n",
      "           1       0.99      0.95      0.97       319\n",
      "\n",
      "    accuracy                           0.98       899\n",
      "   macro avg       0.98      0.97      0.98       899\n",
      "weighted avg       0.98      0.98      0.98       899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Train Binary Classifier\n",
    "pokemon_folder = 'pokemon-data'\n",
    "no_pokemon_folder = 'no-pokemon'\n",
    "binary_images, binary_labels = load_binary_data(pokemon_folder, no_pokemon_folder)\n",
    "\n",
    "# Combine SIFT with other features (optional)\n",
    "sift_features = create_sift_features(binary_images)\n",
    "combined_features = np.hstack([extract_features(binary_images), sift_features])\n",
    "\n",
    "# For binary classifier\n",
    "scaler_bin = StandardScaler()\n",
    "X_binary = scaler_bin.fit_transform(combined_features)\n",
    "y_binary = binary_labels\n",
    "\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.2, stratify=y_binary, random_state=42)\n",
    "\n",
    "binary_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "binary_clf.fit(X_train_bin, y_train_bin)\n",
    "\n",
    "# Evaluate Binary Classifier\n",
    "y_pred_bin = binary_clf.predict(X_test_bin)\n",
    "print(\"Binary Classifier Performance:\")\n",
    "print(classification_report(y_test_bin, y_pred_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\i.kasionis\\AppData\\Local\\anaconda3\\envs\\msc\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1955: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 2560 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Classifier Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Blastoise       0.72      0.72      0.72        29\n",
      "   Bulbasaur       0.87      0.96      0.92        28\n",
      "   Charizard       0.67      0.50      0.57        20\n",
      "  Charmander       0.74      0.71      0.72        24\n",
      "  Charmeleon       0.96      0.90      0.93        29\n",
      "     Ivysaur       0.74      0.87      0.80        30\n",
      "     Pikachu       0.92      0.85      0.88        39\n",
      "      Raichu       0.67      0.85      0.75        33\n",
      "    Squirtle       0.95      0.65      0.77        31\n",
      "    Venusaur       0.79      0.68      0.73        28\n",
      "   Wartortle       0.67      0.86      0.75        28\n",
      "\n",
      "    accuracy                           0.79       319\n",
      "   macro avg       0.79      0.78      0.78       319\n",
      "weighted avg       0.80      0.79      0.79       319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Train Multi-class Classifier (WITH LABEL ENCODING)\n",
    "multi_images, multi_labels = load_multiclass_data(pokemon_folder)\n",
    "\n",
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_multi_encoded = label_encoder.fit_transform(multi_labels)\n",
    "\n",
    "# Combine SIFT with other features (optional)\n",
    "sift_features = create_sift_features(multi_images)\n",
    "combined_features = np.hstack([extract_features(multi_images), sift_features])\n",
    "\n",
    "# For multi-class classifier\n",
    "scaler_multi = StandardScaler()\n",
    "X_multi = scaler_multi.fit_transform(combined_features)\n",
    "\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X_multi, y_multi_encoded, test_size=0.2, stratify=y_multi_encoded, random_state=42\n",
    ")\n",
    "\n",
    "multi_clf = XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "multi_clf.fit(X_train_multi, y_train_multi)\n",
    "\n",
    "# Evaluate Multi-class Classifier\n",
    "y_pred_multi_encoded = multi_clf.predict(X_test_multi)\n",
    "y_pred_multi = label_encoder.inverse_transform(y_pred_multi_encoded)  # Convert back to names\n",
    "\n",
    "print(\"Multi-class Classifier Performance:\")\n",
    "print(classification_report(label_encoder.inverse_transform(y_test_multi), y_pred_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image, min_area=500):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Improved preprocessing\n",
    "    blurred = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, \n",
    "                                  cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                  cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    \n",
    "    # Morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
    "    closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=3)\n",
    "    \n",
    "    # Edge-aware refinement\n",
    "    edges = cv2.Canny(closed, 50, 150)\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    rois = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > min_area:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            # Add padding to capture full shapes\n",
    "            pad = 5\n",
    "            rois.append(image[max(0,y-pad):min(y+h+pad, image.shape[0]),\n",
    "                             max(0,x-pad):min(x+w+pad, image.shape[1])])\n",
    "    return rois\n",
    "\n",
    "# Updated Inference Pipeline\n",
    "def predict_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return []\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Binary classification on full image\n",
    "    img_resized = cv2.resize(img_rgb, TARGET_SIZE)\n",
    "    features = extract_features([img_resized])[0]\n",
    "    sift_features = create_sift_features([img_resized])[0]\n",
    "    combined_features = np.hstack([features, sift_features])\n",
    "    if binary_clf.predict([combined_features])[0] == 0:\n",
    "        return []\n",
    "    \n",
    "    # Segmentation and classification\n",
    "    rois = segment_image(img_rgb)\n",
    "    predictions = []\n",
    "    for roi in rois:\n",
    "        roi_resized = cv2.resize(roi, TARGET_SIZE)\n",
    "        roi_features = extract_features([roi_resized])[0]\n",
    "        sift_features = create_sift_features([roi_resized])[0]\n",
    "        combined_features = np.hstack([roi_features, sift_features])\n",
    "        if binary_clf.predict([combined_features])[0] == 1:\n",
    "            pred_encoded = multi_clf.predict([combined_features])[0]\n",
    "            pred = label_encoder.inverse_transform([pred_encoded])[0]  # Decode label\n",
    "            predictions.append(pred)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\i.kasionis\\AppData\\Local\\anaconda3\\envs\\msc\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1955: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 2560 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_samples=19 should be >= n_clusters=50.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m test_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predict_image(test_image_path))\n",
      "Cell \u001b[1;32mIn[56], line 50\u001b[0m, in \u001b[0;36mpredict_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     48\u001b[0m roi_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(roi, TARGET_SIZE)\n\u001b[0;32m     49\u001b[0m roi_features \u001b[38;5;241m=\u001b[39m extract_features([roi_resized])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 50\u001b[0m sift_features \u001b[38;5;241m=\u001b[39m create_sift_features([roi_resized])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     51\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([roi_features, sift_features])\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m binary_clf\u001b[38;5;241m.\u001b[39mpredict([combined_features])[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[47], line 49\u001b[0m, in \u001b[0;36mcreate_sift_features\u001b[1;34m(images, n_clusters)\u001b[0m\n\u001b[0;32m     47\u001b[0m all_descriptors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(descriptors)\n\u001b[0;32m     48\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m MiniBatchKMeans(n_clusters\u001b[38;5;241m=\u001b[39mn_clusters, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(all_descriptors)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Create histograms\u001b[39;00m\n\u001b[0;32m     52\u001b[0m sift_features \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\i.kasionis\\AppData\\Local\\anaconda3\\envs\\msc\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\i.kasionis\\AppData\\Local\\anaconda3\\envs\\msc\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:2081\u001b[0m, in \u001b[0;36mMiniBatchKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   2047\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the centroids on X by chunking it into mini-batches.\u001b[39;00m\n\u001b[0;32m   2048\u001b[0m \n\u001b[0;32m   2049\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2073\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   2074\u001b[0m     X,\n\u001b[0;32m   2075\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2078\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2079\u001b[0m )\n\u001b[1;32m-> 2081\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   2082\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m   2083\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\i.kasionis\\AppData\\Local\\anaconda3\\envs\\msc\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1923\u001b[0m, in \u001b[0;36mMiniBatchKMeans._check_params_vs_input\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m-> 1923\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X, default_n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m   1925\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;66;03m# init_size\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.kasionis\\AppData\\Local\\anaconda3\\envs\\msc\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:878\u001b[0m, in \u001b[0;36m_BaseKMeans._check_params_vs_input\u001b[1;34m(self, X, default_n_init)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, default_n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;66;03m# n_clusters\u001b[39;00m\n\u001b[0;32m    877\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters:\n\u001b[1;32m--> 878\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    879\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be >= n_clusters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;66;03m# tol\u001b[39;00m\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tol \u001b[38;5;241m=\u001b[39m _tolerance(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol)\n",
      "\u001b[1;31mValueError\u001b[0m: n_samples=19 should be >= n_clusters=50."
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "test_image_path = 'test_image.png'\n",
    "print(\"Predictions:\", predict_image(test_image_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
